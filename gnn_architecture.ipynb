{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1ac542-fb0a-44d6-86f8-c93df0eb8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('GRUCell') != -1:\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "                \n",
    "class GNN_predict(nn.Module):\n",
    "    def __init__(self, h_size, obs_dim, num_actions, inject_action=False):\n",
    "        super(GNN_predict,self).__init__()\n",
    "        if inject_action:\n",
    "            self.l1 = nn.Linear(h_size+num_actions,64)\n",
    "        else:\n",
    "            self.l1 = nn.Linear(h_size, 64)\n",
    "\n",
    "        self.l2 = nn.Linear(64,128)\n",
    "        self.l3 = nn.Linear(128,obs_dim)\n",
    "        self.apply(weights_init)\n",
    "    def forward(self,h):\n",
    "        h = torch.relu(self.l1(h))\n",
    "        h = torch.relu(self.l2(h))\n",
    "        obs = self.l3(h)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1cacd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7380fc-ec2f-449a-92a2-c17df1d529fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, global_mean_pool\n",
    "from torch_geometric.data import Batch, HeteroData\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from graph_utils import split_trajectory_into_steps, create_trajectory_graph\n",
    "\n",
    "\n",
    "\n",
    "class HeteroGNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_channels=64, out_channels=128, num_layers=2, metadata=None):\n",
    "        \"\"\"\n",
    "        Initializes the Heterogeneous GNN Encoder.\n",
    "\n",
    "        Args:\n",
    "            hidden_channels (int): Number of hidden units in GNN layers.\n",
    "            out_channels (int): Dimension of the output latent vector.\n",
    "            num_layers (int): Number of GNN layers.\n",
    "            metadata (tuple): Metadata for HeteroConv, typically (node_types, edge_types).\n",
    "        \"\"\"\n",
    "        super(HeteroGNNEncoder, self).__init__()\n",
    "        \n",
    "        if metadata is None:\n",
    "            raise ValueError(\"Metadata must be provided for HeteroConv.\")\n",
    "        \n",
    "        node_types, edge_types = metadata\n",
    "        self.node_types = node_types\n",
    "        self.edge_types = edge_types\n",
    "        \n",
    "        # Define HeteroConv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv_dict = {}\n",
    "            for edge_type in edge_types:\n",
    "                src, rel, dst = edge_type\n",
    "                conv_dict[edge_type] = SAGEConv(-1, hidden_channels, aggr='mean')\n",
    "            hetero_conv = HeteroConv(conv_dict, aggr='sum')\n",
    "            self.convs.append(hetero_conv)\n",
    "        \n",
    "        # Linear layer to project to latent space\n",
    "        self.linear = nn.Linear(hidden_channels, out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            data (data): A data of HeteroData graphs.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Latent representations of shape (num_graphs, out_channels).\n",
    "        \"\"\"\n",
    "        x_dict = data.x_dict  # Dict of node_type -> node_features\n",
    "\n",
    "        # print(\"x_dict\", x_dict[\"author\"].shape)\n",
    "        # print()\n",
    "        # in the beginning author shape was (200,32),\n",
    "        # because we have 10 author nodes per graph, and we data size 4 and sequence size 5, so 10*4*5=200\n",
    "        edge_index_dict = data.edge_index_dict  # Dict of edge_type -> edge_index\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)  # Perform HeteroConv\n",
    "            x_dict = {key: self.activation(x) for key, x in x_dict.items()}  # Apply activation\n",
    "\n",
    "        # after covilution for every node type shape should be (200,64)\n",
    "        \n",
    "       \n",
    "       \n",
    "        \n",
    "        # for every node type we take a mean of all of the nodes from one graph\n",
    "        x_dict = {key: global_mean_pool(x_dict[key], data[key].batch) for key in data.node_types}\n",
    "        # print(\"shape after pooling for author\")\n",
    "        # print(x_dict[\"author\"].shape) #(20, 64) so for every graph - 64 vector \n",
    "        # print(\"shape after pooling for paper\")\n",
    "        # print(x_dict[\"paper\"].shape) #(20, 64) so for every graph - 64 vector\n",
    "        # print(\"shape after pooling for institution\")\n",
    "        # print(x_dict[\"institution\"].shape) #(20, 64) so for every graph - 64 vector\n",
    "        # # \n",
    "        # print(\"AAAAAA\")\n",
    "        # print(x_dict.values())\n",
    "\n",
    "        # here we connect along the 0 dimension, creating extra dimension\n",
    "        # `stack` creates extra dimension, while `cat` connects along a certain dim\n",
    "        # after stack we get [3, 20, 64], which is [node_types_num, num_of_graphs, hidden_size]\n",
    "        # then we are summing up every element along the node_type dimension. So, sum up all the node types.\n",
    "        # we should get again [20, 64] shape. For every graph, we get 64 size vector\n",
    "        graph_emb = torch.stack(list(x_dict.values()), dim=0).sum(dim=0)\n",
    "        # print(f\"{graph_emb.shape=}\")\n",
    "        \n",
    "        # # Global mean pooling\n",
    "        # graph_emb = global_mean_pool(combined_x, combined_batch)  # Shape: (num_graphs, hidden_channels)\n",
    "        \n",
    "        # Project to desired latent dimension\n",
    "        out = self.linear(graph_emb)  # Shape: (num_graphs, out_channels)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SequenceGNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_channels=64, out_channels=128, num_layers=2, metadata=None):\n",
    "        \"\"\"\n",
    "        Initializes the Sequence GNN Encoder.\n",
    "\n",
    "        Args:\n",
    "            hidden_channels (int): Number of hidden units in GNN layers.\n",
    "            out_channels (int): Dimension of the output latent vector.\n",
    "            num_layers (int): Number of GNN layers.\n",
    "            metadata (tuple): Metadata for HeteroConv, typically (node_types, edge_types).\n",
    "        \"\"\"\n",
    "        super(SequenceGNNEncoder, self).__init__()\n",
    "        self.encoder = HeteroGNNEncoder(hidden_channels, out_channels, num_layers, metadata)\n",
    "        \n",
    "    def forward(self, graphs_batch):\n",
    "        \"\"\"\n",
    "        Forward pass for a batch of graph sequences.\n",
    "\n",
    "        Args:\n",
    "            graphs_batch (list of list of HeteroData): \n",
    "                Outer list has length batch_size.\n",
    "                Each inner list has length sequence_size, containing HeteroData graphs.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Latent representations of shape (batch_size, sequence_size, out_channels).\n",
    "        \"\"\"\n",
    "        batch_size = len(graphs_batch)\n",
    "        sequence_size = len(graphs_batch[0])\n",
    "        \n",
    "        # Flatten the list of lists into a single list\n",
    "        all_graphs = [graph for batch in graphs_batch for graph in batch]\n",
    "        \n",
    "        # Create a Batch object from the flattened list\n",
    "        batch = Batch.from_data_list(all_graphs)\n",
    "        \n",
    "        # Encode all graphs\n",
    "        encoded = self.encoder(batch)  # Shape: (batch_size * sequence_size, out_channels)\n",
    "        \n",
    "        # Reshape to (batch_size, sequence_size, out_channels)\n",
    "        encoded = encoded.view(batch_size, sequence_size, -1)\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example metadata: define node types and edge types\n",
    "    node_types = ['author', 'paper', 'institution']\n",
    "    edge_types = [\n",
    "        ('author', 'writes', 'paper'),\n",
    "        ('paper', 'cites', 'paper'),\n",
    "        ('paper', 'affiliated_with', 'institution'),\n",
    "        ('institution', 'hosts', 'author')\n",
    "    ]\n",
    "    metadata = (node_types, edge_types)\n",
    "    \n",
    "    # Initialize the encoder\n",
    "    encoder = SequenceGNNEncoder(hidden_channels=64, out_channels=128, num_layers=2, metadata=metadata)\n",
    "    \n",
    "    # Create dummy data\n",
    "    # For simplicity, create random features and random edges\n",
    "    def create_dummy_heterodata():\n",
    "        data = HeteroData()\n",
    "        if torch.randint(0, 10, (1,)) > 5:\n",
    "            # print(\"1\")\n",
    "            # Example node features\n",
    "            data['author'].x = torch.randn(10, 32)       # 10 authors with 32-dim features\n",
    "            data['paper'].x = torch.randn(20, 64)        # 20 papers with 64-dim features\n",
    "            data['institution'].x = torch.randn(5, 16)   # 5 institutions with 16-dim features\n",
    "        else:\n",
    "            # print(\"2\")\n",
    "             # Example node features\n",
    "            data['author'].x = torch.randn(9, 32)       # 9 authors with 32-dim features\n",
    "            data['paper'].x = torch.randn(19, 64)        # 19 papers with 64-dim features\n",
    "            data['institution'].x = torch.randn(4, 16)   # 4 institutions with 16-dim features\n",
    "        \n",
    "             \n",
    "        \n",
    "        # Example edges\n",
    "        data['author', 'writes', 'paper'].edge_index = torch.tensor([\n",
    "            [0, 1, 2, 3],\n",
    "            [0, 1, 2, 3]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        data['paper', 'cites', 'paper'].edge_index = torch.tensor([\n",
    "            [0, 1, 2],\n",
    "            [1, 2, 3]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        data['paper', 'affiliated_with', 'institution'].edge_index = torch.tensor([\n",
    "            [0, 1, 2],\n",
    "            [0, 1, 2]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        data['institution', 'hosts', 'author'].edge_index = torch.tensor([\n",
    "            [0, 1],\n",
    "            [0, 1]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    batch_size = 4\n",
    "    sequence_size = 5\n",
    "    \n",
    "    # Create a batch of sequences\n",
    "    graphs_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        sequence = [create_dummy_heterodata() for _ in range(sequence_size)]\n",
    "        graphs_batch.append(sequence)\n",
    "    \n",
    "    # Move to device if needed\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = encoder.to(device)\n",
    "    \n",
    "    # Optionally, move data to device\n",
    "    # This requires iterating and moving each HeteroData to the device\n",
    "    for i in range(batch_size):\n",
    "        for j in range(sequence_size):\n",
    "            for key in graphs_batch[i][j].x_dict.keys():\n",
    "                graphs_batch[i][j].x_dict[key] = graphs_batch[i][j].x_dict[key].to(device)\n",
    "            for key in graphs_batch[i][j].edge_index_dict.keys():\n",
    "                graphs_batch[i][j].edge_index_dict[key] = graphs_batch[i][j].edge_index_dict[key].to(device)\n",
    "    \n",
    "    # Encode the batch\n",
    "    with torch.no_grad():\n",
    "        latent_representations = encoder(graphs_batch)  # Shape: (batch_size, sequence_size, 128)\n",
    "    \n",
    "    print(latent_representations.shape)  # Should print torch.Size([4, 5, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6c31fe-6440-4ca0-8580-02166a17e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GNN_predict(h_size=latent_representations.shape[-1], obs_dim=34, num_actions=25, inject_action=False)\n",
    "pred = decoder(latent_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed5ed77-1833-4c5b-b6e9-9678b7b6c9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 34])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e50fb2-aa6d-4965-bc23-6e73b2a9932a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c3cb1f3-7a6e-4837-9941-68af2be54607",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = torch.tensor((\n",
    "    [\n",
    "    [1,2,3],[4,5,6], [1,2,3],[4,5,6]\n",
    "    ],\n",
    "    [\n",
    "    [11,12,13],[41,53,60],[31,42,53],[46,5,16]\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bba970-0c80-4152-809f-5f10acf1f168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc0a7f9b-60a2-40b7-a5e1-f2007e4d7e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 14, 16],\n",
       "        [45, 58, 66],\n",
       "        [32, 44, 56],\n",
       "        [50, 10, 22]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "218e8494-9bd1-4278-a769-7418d5fee513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_loop():\n",
    "#     epoch_0 = 0\n",
    "#     for epoch in range(epoch_0, self.autoencoder_num_epochs):\n",
    "#                 epoch_loss = []\n",
    "#                 print(\"Experiment: autoencoder {0}: training Epoch = \".format(self.autoencoder), epoch+1, 'out of', self.autoencoder_num_epochs, 'epochs')\n",
    "    \n",
    "#                 # Loop through all the train data using the data loader\n",
    "#                 for ii, (dem, ob, ac, l, t, scores, rewards, idx) in enumerate(self.train_loader):\n",
    "#                     # print(\"Batch {}\".format(ii),end='')\n",
    "#                     dem = dem.to(device)  # 5 dimensional vector (Gender, Ventilation status, Re-admission status, Age, Weight)\n",
    "#                     ob = ob.to(device)    # 33 dimensional vector (time varying measures)\n",
    "#                     ac = ac.to(device) # actions\n",
    "#                     l = l.to(device)\n",
    "#                     t = t.to(device)\n",
    "#                     scores = scores.to(device)\n",
    "#                     idx = idx.to(device)\n",
    "#                     loss_pred = 0\n",
    "    \n",
    "#                     # Cut tensors down to the batch's largest sequence length... Trying to speed things up a bit...\n",
    "#                     max_length = int(l.max().item())\n",
    "    \n",
    "#                     # The following losses are for DDM and will not be modified by any other approach\n",
    "#                     train_loss, dec_loss, inv_loss = 0, 0, 0\n",
    "#                     model_loss, recon_loss, forward_loss = 0, 0, 0                    \n",
    "                        \n",
    "#                     # Set training mode (nn.Module.train()). It does not actually trains the model, but just sets the model to training mode.\n",
    "#                     self.gen.train()\n",
    "#                     self.pred.train()\n",
    "    \n",
    "#                     ob = ob[:,:max_length,:]\n",
    "#                     dem = dem[:,:max_length,:]\n",
    "#                     ac = ac[:,:max_length,:]\n",
    "#                     scores = scores[:,:max_length,:]\n",
    "                    \n",
    "#                     # Special case for CDE\n",
    "#                     # Getting loss_pred and mse_loss\n",
    "#                     if self.autoencoder == 'CDE':\n",
    "#                         loss_pred, mse_loss, _ = self.container.loop(ob, dem, ac, scores, l, max_length, self.context_input, corr_coeff_param = self.corr_coeff_param, device = device, coefs = self.train_coefs, idx = idx)\n",
    "#                     else:\n",
    "#                         loss_pred, mse_loss, _ = self.container.loop(ob, dem, ac, scores, l, max_length, self.context_input, corr_coeff_param = self.corr_coeff_param, device=device, autoencoder = self.autoencoder)   \n",
    "    \n",
    "#                     self.optimizer.zero_grad()\n",
    "                    \n",
    "#                     if self.autoencoder != 'DDM':\n",
    "#                         loss_pred.backward()\n",
    "#                         self.optimizer.step()\n",
    "#                         epoch_loss.append(loss_pred.detach().cpu().numpy())                \n",
    "#                     else:\n",
    "#                         # For DDM\n",
    "#                         train_loss, dec_loss, inv_loss, model_loss, recon_loss, forward_loss, corr_loss, loss_pred = loss_pred\n",
    "#                         train_loss = forward_loss + self.inv_loss_coef*inv_loss + self.dec_loss_coef*dec_loss - self.corr_coeff_param*corr_loss.sum()\n",
    "#                         train_loss.backward()\n",
    "#                         # Clipping gradients to prevent exploding gradients\n",
    "#                         torch.nn.utils.clip_grad_norm(self.all_params, self.max_grad_norm)\n",
    "#                         self.optimizer.step()\n",
    "#                         epoch_loss.append(loss_pred.detach().cpu().numpy())\n",
    "                                            \n",
    "#                 self.autoencoding_losses.append(epoch_loss)\n",
    "#                 if (epoch+1)%self.saving_period == 0: # Run validation and also save checkpoint\n",
    "                    \n",
    "#                     #Computing validation loss\n",
    "#                     epoch_validation_loss = []\n",
    "#                     with torch.no_grad():\n",
    "#                         for jj, (dem, ob, ac, l, t, scores, rewards, idx) in enumerate(self.val_loader):\n",
    "    \n",
    "#                             dem = dem.to(device)\n",
    "#                             ob = ob.to(device)\n",
    "#                             ac = ac.to(device)\n",
    "#                             l = l.to(device)\n",
    "#                             t = t.to(device)\n",
    "#                             idx = idx.to(device)\n",
    "#                             scores = scores.to(device)\n",
    "#                             loss_val = 0\n",
    "    \n",
    "#                             # Cut tensors down to the batch's largest sequence length... Trying to speed things up a bit...\n",
    "#                             max_length = int(l.max().item())                        \n",
    "                            \n",
    "#                             ob = ob[:,:max_length,:]\n",
    "#                             dem = dem[:,:max_length,:]\n",
    "#                             ac = ac[:,:max_length,:] \n",
    "#                             scores = scores[:,:max_length,:] \n",
    "                            \n",
    "#                             self.gen.eval()\n",
    "#                             self.pred.eval()    \n",
    "                            \n",
    "#                             if self.autoencoder == 'CDE':\n",
    "#                                 loss_val, mse_loss, _ = self.container.loop(ob, dem, ac, scores, l, max_length, corr_coeff_param = 0, device = device, coefs = self.val_coefs, idx = idx)\n",
    "#                             else:\n",
    "#                                 loss_val, mse_loss, _ = self.container.loop(ob, dem, ac, scores, l, max_length, self.context_input, corr_coeff_param = 0, device=device, autoencoder = self.autoencoder)                                                 \n",
    "                            \n",
    "#                             if self.autoencoder in ['DST', 'ODERNN', 'CDE']:\n",
    "#                                 epoch_validation_loss.append(mse_loss)\n",
    "#                             elif self.autoencoder == \"DDM\":\n",
    "#                                 epoch_validation_loss.append(loss_val[-1].detach().cpu().numpy())\n",
    "#                             else:\n",
    "#                                 epoch_validation_loss.append(loss_val.detach().cpu().numpy())\n",
    "                        \n",
    "                            \n",
    "#                     self.autoencoding_losses_validation.append(epoch_validation_loss)\n",
    "    \n",
    "#                     save_dict = {'epoch': epoch,\n",
    "#                             'gen_state_dict': self.gen.state_dict(),\n",
    "#                             'pred_state_dict': self.pred.state_dict(),\n",
    "#                             'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "#                             'loss': self.autoencoding_losses,\n",
    "#                             'validation_loss': self.autoencoding_losses_validation\n",
    "#                             }\n",
    "                    \n",
    "#                     if self.autoencoder == 'DDM':\n",
    "#                         save_dict['dyn_state_dict'] = self.dyn.state_dict()\n",
    "                        \n",
    "#                     try:\n",
    "#                         torch.save(save_dict, self.checkpoint_file)\n",
    "#                         # torch.save(save_dict, self.checkpoint_file[:-3] + str(epoch) +'_.pt')\n",
    "#                         np.save(self.data_folder + '/{}_losses.npy'.format(self.autoencoder.lower()), np.array(self.autoencoding_losses))\n",
    "#                     except Exception as e:\n",
    "#                         print(e)\n",
    "    \n",
    "                    \n",
    "#                     try:\n",
    "#                         np.save(self.data_folder + '/{}_validation_losses.npy'.format(self.autoencoder.lower()), np.array(self.autoencoding_losses_validation))\n",
    "#                     except Exception as e:\n",
    "#                         print(e)\n",
    "                        \n",
    "#                 #Final epoch checkpoint\n",
    "#                 try:\n",
    "#                     save_dict = {\n",
    "#                                 'epoch': self.autoencoder_num_epochs-1,\n",
    "#                                 'gen_state_dict': self.gen.state_dict(),\n",
    "#                                 'pred_state_dict': self.pred.state_dict(),\n",
    "#                                 'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "#                                 'loss': self.autoencoding_losses,\n",
    "#                                 'validation_loss': self.autoencoding_losses_validation,\n",
    "#                                 }\n",
    "#                     if self.autoencoder == 'DDM':\n",
    "#                         save_dict['dyn_state_dict'] = self.dyn.state_dict()\n",
    "#                         torch.save(self.dyn.state_dict(), self.dyn_file)\n",
    "#                     torch.save(self.gen.state_dict(), self.gen_file)\n",
    "#                     torch.save(self.pred.state_dict(), self.pred_file)\n",
    "#                     torch.save(save_dict, self.checkpoint_file)\n",
    "#                     np.save(self.data_folder + '/{}_losses.npy'.format(self.autoencoder.lower()), np.array(self.autoencoding_losses))\n",
    "#                 except Exception as e:\n",
    "#                         print(e)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5f2ca7-a635-4e38-aeb2-20e843f8eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.node_time_dict = {\n",
    "#     \"user\": torch.tensor([1, 3, 5, 7]),  # Timestamps for user nodes\n",
    "#     \"item\": torch.tensor([2, 4, 6, 8]),  # Timestamps for item nodes\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be726955-af7a-4c98-8f10-094388cae55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @timeit\n",
    "# def time_select_up_to_t(dataset, t):\n",
    "#     \"\"\"Select nodes and edges up to and including time step t.\n",
    "#     @param t: time index (inclusive)\n",
    "#     @return: HeteroData, with sliced nodes and edges based on time.\n",
    "#     \"\"\"\n",
    "#     dt = HeteroData()\n",
    "#     d = dataset\n",
    "\n",
    "#     # Get node timestamps (if they exist)\n",
    "#     node_time_dict = getattr(d, \"node_time_dict\", {})  # Use empty dict if node_time_dict doesn't exist\n",
    "\n",
    "#     # Copy node information up to and including time step t\n",
    "#     for ntype, value in d.x_dict.items():\n",
    "#         if ntype in node_time_dict:  # If node type has time information\n",
    "#             mask = (node_time_dict[ntype] <= t).squeeze(-1)  # Filter nodes based on time\n",
    "#             dt[ntype].x = value[mask]  # Copy node features for filtered nodes\n",
    "#             if \"num_nodes\" in d[ntype].keys():\n",
    "#                 dt[ntype].num_nodes = mask.sum().item()  # Update number of nodes\n",
    "#         else:  # If node type has no time information, copy all nodes\n",
    "#             dt[ntype].x = value\n",
    "#             if \"num_nodes\" in d[ntype].keys():\n",
    "#                 dt[ntype].num_nodes = d[ntype].num_nodes\n",
    "\n",
    "#     # Get edge timestamps and edge indices\n",
    "#     dea = d.edge_time_dict\n",
    "    # dei = d.edge_index_dict\n",
    "\n",
    "    # # Filter edges up to and including time step t\n",
    "    # for etype in dea:\n",
    "    #     mask = (dea[etype] <= t).squeeze(-1)  # Include all edges with time <= t\n",
    "    #     dt[etype].edge_index = dei[etype][:, mask]\n",
    "    #     dt[etype].edge_time = dea[etype][mask]\n",
    "\n",
    "    # return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f8a1ce-d667-4a7e-a93b-5fc13217d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file =  '/Users/taiskha/Master Thesis/code/data/train_set_tuples'\n",
    "minibatch_size = 128\n",
    "latent_dim = 128\n",
    "num_actions = 25\n",
    "new_obs_dim = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daa6c478-3c9b-468f-b135-d0871707cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(obs, dem, actions, l, device='cpu', **kwargs):\n",
    "    '''This loop through the training and validation data is the general template for AIS, RNN, etc'''\n",
    "    # Split the observations \n",
    "    # autoencoder = kwargs['autoencoder']\n",
    "\n",
    "    # Transfer weight property from dem to obs, because weight changes over time\n",
    "\n",
    "    # Remove the last column from dem\n",
    "    demography_new = dem[:, :, :-1]  # Shape: (128, 20, 4)\n",
    "    \n",
    "    # Extract the last column from dem\n",
    "    demography_last_column = dem[:, :, -1:]  # Shape: (128, 20, 1)\n",
    "\n",
    "    # Append the last column to obs\n",
    "    obs_new = torch.cat([obs, demography_last_column], dim=2)  # Shape: (128, 20, 33+1=34)\n",
    "\n",
    "    obs = obs_new\n",
    "    dem = demography_new\n",
    "    \n",
    "\n",
    "    \n",
    "    cur_obs, next_obs = obs[:,:-1,:], obs[:,1:,:]\n",
    "    # print(f\"{cur_obs.shape=}\") # (128, 19, 34)\n",
    "    # print(f\"{next_obs.shape=}\") # (128, 19, 34)\n",
    "    cur_dem = dem[:,:-1,:] \n",
    "    # print(f\"{cur_dem.shape=}\")# (128, 19, 4)\n",
    "    \n",
    "    \n",
    "    # We need to cut the actions, too\n",
    "    curr_actions = actions[:,:-1,:] \n",
    "    # And we need to subtract 1 from every length, because we cut one element from every trajectory in the minbatch\n",
    "    curr_l = l - 1\n",
    "    \n",
    "    # cur_scores, next_scores = scores[:,:-1,:], scores[:,1:,:] # I won't need the \"next scores\"\n",
    "    mask = (cur_obs ==0).all(dim=2) # Compute mask for extra appended rows of observations (all zeros along dim 2)\n",
    "    # print(f\"{l.shape=}\")\n",
    "    # print(f\"{t[0]=}\")\n",
    "\n",
    "\n",
    "    data = (cur_dem, cur_obs, curr_actions, curr_l)\n",
    "    batch_full_trajectory_graphs, batch_lengths = create_trajectory_graph(data)\n",
    "    # print(len(batch_full_trajectory_graphs))\n",
    "    graphs_batch = split_trajectory_into_steps(batch_full_trajectory_graphs, batch_lengths)\n",
    "    # print(len(graphs_batch),len(graphs_batch[0]))\n",
    "    # print(graphs_batch[0][0])\n",
    "    \n",
    "    metadata = graphs_batch[0][1].metadata()\n",
    "    print(metadata)\n",
    "    node_types, edge_types = metadata\n",
    "    # print(node_types)\n",
    "    # print(edge_types)\n",
    "\n",
    "    encoder = SequenceGNNEncoder(hidden_channels=64, out_channels=latent_dim, num_layers=2, metadata=metadata)\n",
    "    decoder = GNN_predict(h_size=latent_dim, obs_dim=new_obs_dim, num_actions=num_actions, inject_action=False)\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    \n",
    "\n",
    "    # Optionally, move data to device\n",
    "    # This requires iterating and moving each HeteroData to the device\n",
    "    for i in range(len(graphs_batch)):\n",
    "        for j in range(sequence_size):\n",
    "            for key in graphs_batch[i][j].x_dict.keys():\n",
    "                graphs_batch[i][j].x_dict[key] = graphs_batch[i][j].x_dict[key].to(device)\n",
    "            for key in graphs_batch[i][j].edge_index_dict.keys():\n",
    "                graphs_batch[i][j].edge_index_dict[key] = graphs_batch[i][j].edge_index_dict[key].to(device)\n",
    "    \n",
    "    # Encode the batch\n",
    "    with torch.no_grad():\n",
    "        latent_representations = encoder(graphs_batch)  # Shape: (batch_size, sequence_size, 128)\n",
    "    \n",
    "    print(latent_representations.shape)  # Should print torch.Size([128, 20, 128])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_obs = decoder(latent_representations)\n",
    "\n",
    "    print(pred_obs.shape)\n",
    "\n",
    "    assert pred_obs.shape == next_obs.shape\n",
    "    \n",
    "    \n",
    "\n",
    "    # This concatenates an empty action with the first observation and shifts all actions \n",
    "    # to the next observation since we're interested in pairing obs with previous action\n",
    "    # if context_input:\n",
    "    #     hidden_states = self.gen(torch.cat((cur_obs, cur_dem, torch.cat((torch.zeros((obs.shape[0],1,actions.shape[-1])).to(device),actions[:,:-2,:]),dim=1)),dim=-1))\n",
    "    # else:\n",
    "    #     hidden_states = self.gen(torch.cat((cur_obs, torch.cat((torch.zeros((obs.shape[0],1,actions.shape[-1])).to(device), actions[:,:-2,:]),dim=1)), dim=-1))\n",
    "\n",
    "    # print(f\"{hidden_states.shape=}\")\n",
    "    # print(f\"{actions.shape=}\")\n",
    "\n",
    "    # For RNN autoencoder, we only need the hidden states as input to the decoder\n",
    "    # For the others, we need to concatenate the hidden states with the actions\n",
    "    # if autoencoder == 'RNN':\n",
    "    #     pred_obs = self.pred(hidden_states)\n",
    "    # else:\n",
    "    #     pred_obs = self.pred(torch.cat((hidden_states,actions[:,:-1,:]),dim=-1))\n",
    "\n",
    "    # print(f\"{pred_obs.shape=}\")\n",
    "\n",
    "    # Calculate the correlation between the hidden parameters and the acuity score (For now we'll use SOFA--idx 0)\n",
    "    # corr_loss = pearson_correlation(hidden_states[~mask], scores[:,:-1,:][~mask], device=device)\n",
    "    temp_loss = -torch.distributions.MultivariateNormal(pred_obs, torch.eye(pred_obs.shape[-1]).to(device)).log_prob(next_obs)\n",
    "    mse_loss = sum(temp_loss[~mask])\n",
    "    # loss_pred = mse_loss - corr_coeff_param*corr_loss.sum() # We only want to keep the relevant rows of the loss, sum them up! We then add the scaled correlation coefficient\n",
    "    # We do not care that the latent representations correlate with acuity scores\n",
    "    loss_pred = mse_loss\n",
    "\n",
    "    return loss_pred, mse_loss, latent_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b61387b-b088-4c56-a58a-98597f78045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['patient', 'timestep'], [('patient', 'has_timestep', 'timestep'), ('timestep', 'has_timestep', 'patient'), ('timestep', 'action', 'timestep')])\n",
      "torch.Size([128, 19, 128])\n",
      "torch.Size([128, 19, 34])\n"
     ]
    }
   ],
   "source": [
    "train_demog, train_states, train_interventions, train_lengths, train_times, acuities, rewards = torch.load(train_data_file)\n",
    "train_idx = torch.arange(train_demog.shape[0])\n",
    "train_dataset = TensorDataset(train_demog, train_states, train_interventions,train_lengths,train_times, acuities, rewards, train_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "for ii, (dem, ob, ac, l, t, scores, rewards, idx) in enumerate(train_loader):\n",
    "                if ii > 0:\n",
    "                    break\n",
    "                # print(\"Batch {}\".format(ii),end='')\n",
    "    \n",
    "                # we've got 128 different trajectories of different length\n",
    "    \n",
    "                dem = dem.to(device)  # 5 dimensional vector (Gender, Ventilation status, Re-admission status, Age, Weight)\n",
    "                ob = ob.to(device)    # 33 dimensional vector (time varying measures)\n",
    "                ac = ac.to(device) # actions\n",
    "                l = l.to(device)\n",
    "                t = t.to(device)\n",
    "                scores = scores.to(device)\n",
    "                idx = idx.to(device)\n",
    "                loss_pred = 0\n",
    "\n",
    "                # Cut tensors down to the batch's largest sequence length... Trying to speed things up a bit...\n",
    "                max_length = int(l.max().item())\n",
    "                min_length = int(l.min().item())\n",
    "                # print(f\"{max_length=}\")\n",
    "                # print(f\"{min_length=}\")\n",
    "                # # The following losses are for DDM and will not be modified by any other approach\n",
    "                # train_loss, dec_loss, inv_loss = 0, 0, 0\n",
    "                # model_loss, recon_loss, forward_loss = 0, 0, 0                    \n",
    "                    \n",
    "                # # Set training mode (nn.Module.train()). It does not actually trains the model, but just sets the model to training mode.\n",
    "                # self.gen.train()\n",
    "                # self.pred.train()\n",
    "                \n",
    "                ob = ob[:,:max_length,:]\n",
    "                dem = dem[:,:max_length,:]\n",
    "                ac = ac[:,:max_length,:]\n",
    "                scores = scores[:,:max_length,:]\n",
    "\n",
    "                loss_pred, mse_loss, hidden_states = loop(obs=ob, dem=dem, actions=ac, l=l, device=device)\n",
    "\n",
    "                # data = (dem, ob, ac, l, t, scores, rewards, idx)\n",
    "                # batch_full_trajectory_graphs, batch_lengths = create_trajectory_graph(data)\n",
    "                # print(len(batch_full_trajectory_graphs))\n",
    "                # graphs_batch = split_trajectory_into_steps(batch_full_trajectory_graphs, batch_lengths)\n",
    "                # print(len(graphs_batch))\n",
    "                \n",
    "                # metadata = graphs_batch[0][0].metadata()\n",
    "                # node_types, edge_types = metadata\n",
    "                # # print(node_types)\n",
    "                # # print(edge_types)\n",
    "\n",
    "                # encoder = SequenceGNNEncoder(hidden_channels=64, out_channels=latent_dim, num_layers=2, metadata=metadata)\n",
    "                # decoder = GNN_predict(h_size=latent_dim, obs_dim=new_obs_dim, num_actions=num_actions, inject_action=False)\n",
    "                # encoder = encoder.to(device)\n",
    "                # decoder = decoder.to(device)\n",
    "                \n",
    "    \n",
    "                # # Optionally, move data to device\n",
    "                # # This requires iterating and moving each HeteroData to the device\n",
    "                # for i in range(minibatch_size):\n",
    "                #     for j in range(sequence_size):\n",
    "                #         for key in graphs_batch[i][j].x_dict.keys():\n",
    "                #             graphs_batch[i][j].x_dict[key] = graphs_batch[i][j].x_dict[key].to(device)\n",
    "                #         for key in graphs_batch[i][j].edge_index_dict.keys():\n",
    "                #             graphs_batch[i][j].edge_index_dict[key] = graphs_batch[i][j].edge_index_dict[key].to(device)\n",
    "                \n",
    "                # # Encode the batch\n",
    "                # with torch.no_grad():\n",
    "                #     latent_representations = encoder(graphs_batch)  # Shape: (batch_size, sequence_size, 128)\n",
    "                \n",
    "                # print(latent_representations.shape)  # Should print torch.Size([128, 20, 128])\n",
    "\n",
    "                # with torch.no_grad():\n",
    "                #     pred = decoder(latent_representations)\n",
    "\n",
    "                # print(pred.shape)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf615066-5a0e-4ab9-9bb7-2830dc280f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(84525.3828)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a7f00d3-c384-409d-81df-5888bbc5f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for _ in range(batch_size):\n",
    "#         sequence = [create_dummy_heterodata() for _ in range(sequence_size)]\n",
    "#         graphs_batch.append(sequence)\n",
    "# reference_graph = sequence[-1]\n",
    "# edge_type = reference_graph.edge_types[2]\n",
    "# print(edge_type)\n",
    "# print(reference_graph[edge_type].edge_index)\n",
    "# reference_graph[edge_type].edge_attr = torch.randn(3, 5)\n",
    "# reference_graph[edge_type].edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca377efd-e6f4-41c8-bdf2-00a68a461a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1] * 5).view(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78eaa7af-183d-44aa-93b2-70cbace46beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k =torch.tensor([4,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e10bcee-0b15-42f4-8ee9-3aec3dd7ef2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1956882-3ee9-432f-bd8e-6b9d521ba8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name",
   "language": "python",
   "name": "my-virtualenv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
