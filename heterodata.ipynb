{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c7380fc-ec2f-449a-92a2-c17df1d529fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "graph_emb.shape=torch.Size([20, 64])\n",
      "torch.Size([4, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, global_mean_pool\n",
    "from torch_geometric.data import data, HeteroData\n",
    "\n",
    "class HeteroGNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_channels=64, out_channels=128, num_layers=2, metadata=None):\n",
    "        \"\"\"\n",
    "        Initializes the Heterogeneous GNN Encoder.\n",
    "\n",
    "        Args:\n",
    "            hidden_channels (int): Number of hidden units in GNN layers.\n",
    "            out_channels (int): Dimension of the output latent vector.\n",
    "            num_layers (int): Number of GNN layers.\n",
    "            metadata (tuple): Metadata for HeteroConv, typically (node_types, edge_types).\n",
    "        \"\"\"\n",
    "        super(HeteroGNNEncoder, self).__init__()\n",
    "        \n",
    "        if metadata is None:\n",
    "            raise ValueError(\"Metadata must be provided for HeteroConv.\")\n",
    "        \n",
    "        node_types, edge_types = metadata\n",
    "        self.node_types = node_types\n",
    "        self.edge_types = edge_types\n",
    "        \n",
    "        # Define HeteroConv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv_dict = {}\n",
    "            for edge_type in edge_types:\n",
    "                src, rel, dst = edge_type\n",
    "                conv_dict[edge_type] = SAGEConv(-1, hidden_channels, aggr='mean')\n",
    "            hetero_conv = HeteroConv(conv_dict, aggr='sum')\n",
    "            self.convs.append(hetero_conv)\n",
    "        \n",
    "        # Linear layer to project to latent space\n",
    "        self.linear = nn.Linear(hidden_channels, out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            data (data): A data of HeteroData graphs.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Latent representations of shape (num_graphs, out_channels).\n",
    "        \"\"\"\n",
    "        x_dict = data.x_dict  # Dict of node_type -> node_features\n",
    "\n",
    "        # print(\"x_dict\", x_dict[\"author\"].shape)\n",
    "        # print()\n",
    "        # in the beginning author shape was (200,32),\n",
    "        # because we have 10 author nodes per graph, and we data size 4 and sequence size 5, so 10*4*5=200\n",
    "        edge_index_dict = data.edge_index_dict  # Dict of edge_type -> edge_index\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)  # Perform HeteroConv\n",
    "            x_dict = {key: self.activation(x) for key, x in x_dict.items()}  # Apply activation\n",
    "\n",
    "        # after covilution for every node type shape should be (200,64)\n",
    "        \n",
    "       \n",
    "       \n",
    "        \n",
    "        # for every node type we take a mean of all of the nodes from one graph\n",
    "        x_dict = {key: global_mean_pool(x_dict[key], data[key].batch) for key in data.node_types}\n",
    "        # print(\"shape after pooling for author\")\n",
    "        # print(x_dict[\"author\"].shape) #(20, 64) so for every graph - 64 vector \n",
    "        # print(\"shape after pooling for paper\")\n",
    "        # print(x_dict[\"paper\"].shape) #(20, 64) so for every graph - 64 vector\n",
    "        # print(\"shape after pooling for institution\")\n",
    "        # print(x_dict[\"institution\"].shape) #(20, 64) so for every graph - 64 vector\n",
    "        # # \n",
    "        # print(\"AAAAAA\")\n",
    "        # print(x_dict.values())\n",
    "\n",
    "        # here we connect along the 0 dimension, creating extra dimension\n",
    "        # `stack` creates extra dimension, while `cat` connects along a certain dim\n",
    "        # after stack we get [3, 20, 64], which is [node_types_num, num_of_graphs, hidden_size]\n",
    "        # then we are summing up every element along the node_type dimension. So, sum up all the node types.\n",
    "        # we should get again [20, 64] shape. For every graph, we get 64 size vector\n",
    "        graph_emb = torch.stack(list(x_dict.values()), dim=0).sum(dim=0)\n",
    "        # print(f\"{graph_emb.shape=}\")\n",
    "        \n",
    "        # # Global mean pooling\n",
    "        # graph_emb = global_mean_pool(combined_x, combined_batch)  # Shape: (num_graphs, hidden_channels)\n",
    "        \n",
    "        # Project to desired latent dimension\n",
    "        out = self.linear(graph_emb)  # Shape: (num_graphs, out_channels)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SequenceGNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_channels=64, out_channels=128, num_layers=2, metadata=None):\n",
    "        \"\"\"\n",
    "        Initializes the Sequence GNN Encoder.\n",
    "\n",
    "        Args:\n",
    "            hidden_channels (int): Number of hidden units in GNN layers.\n",
    "            out_channels (int): Dimension of the output latent vector.\n",
    "            num_layers (int): Number of GNN layers.\n",
    "            metadata (tuple): Metadata for HeteroConv, typically (node_types, edge_types).\n",
    "        \"\"\"\n",
    "        super(SequenceGNNEncoder, self).__init__()\n",
    "        self.encoder = HeteroGNNEncoder(hidden_channels, out_channels, num_layers, metadata)\n",
    "        \n",
    "    def forward(self, graphs_batch):\n",
    "        \"\"\"\n",
    "        Forward pass for a batch of graph sequences.\n",
    "\n",
    "        Args:\n",
    "            graphs_batch (list of list of HeteroData): \n",
    "                Outer list has length batch_size.\n",
    "                Each inner list has length sequence_size, containing HeteroData graphs.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Latent representations of shape (batch_size, sequence_size, out_channels).\n",
    "        \"\"\"\n",
    "        batch_size = len(graphs_batch)\n",
    "        sequence_size = len(graphs_batch[0])\n",
    "        \n",
    "        # Flatten the list of lists into a single list\n",
    "        all_graphs = [graph for batch in graphs_batch for graph in batch]\n",
    "        \n",
    "        # Create a Batch object from the flattened list\n",
    "        batch = Batch.from_data_list(all_graphs)\n",
    "        \n",
    "        # Encode all graphs\n",
    "        encoded = self.encoder(batch)  # Shape: (batch_size * sequence_size, out_channels)\n",
    "        \n",
    "        # Reshape to (batch_size, sequence_size, out_channels)\n",
    "        encoded = encoded.view(batch_size, sequence_size, -1)\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example metadata: define node types and edge types\n",
    "    node_types = ['author', 'paper', 'institution']\n",
    "    edge_types = [\n",
    "        ('author', 'writes', 'paper'),\n",
    "        ('paper', 'cites', 'paper'),\n",
    "        ('paper', 'affiliated_with', 'institution'),\n",
    "        ('institution', 'hosts', 'author')\n",
    "    ]\n",
    "    metadata = (node_types, edge_types)\n",
    "    \n",
    "    # Initialize the encoder\n",
    "    encoder = SequenceGNNEncoder(hidden_channels=64, out_channels=128, num_layers=2, metadata=metadata)\n",
    "    \n",
    "    # Create dummy data\n",
    "    # For simplicity, create random features and random edges\n",
    "    def create_dummy_heterodata():\n",
    "        data = HeteroData()\n",
    "        if torch.randint(0, 10, (1,)) > 5:\n",
    "            # print(\"1\")\n",
    "            # Example node features\n",
    "            data['author'].x = torch.randn(10, 32)       # 10 authors with 32-dim features\n",
    "            data['paper'].x = torch.randn(20, 64)        # 20 papers with 64-dim features\n",
    "            data['institution'].x = torch.randn(5, 16)   # 5 institutions with 16-dim features\n",
    "        else:\n",
    "            # print(\"2\")\n",
    "             # Example node features\n",
    "            data['author'].x = torch.randn(9, 32)       # 9 authors with 32-dim features\n",
    "            data['paper'].x = torch.randn(19, 64)        # 19 papers with 64-dim features\n",
    "            data['institution'].x = torch.randn(4, 16)   # 4 institutions with 16-dim features\n",
    "        \n",
    "             \n",
    "        \n",
    "        # Example edges\n",
    "        data['author', 'writes', 'paper'].edge_index = torch.tensor([\n",
    "            [0, 1, 2, 3],\n",
    "            [0, 1, 2, 3]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        data['paper', 'cites', 'paper'].edge_index = torch.tensor([\n",
    "            [0, 1, 2],\n",
    "            [1, 2, 3]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        data['paper', 'affiliated_with', 'institution'].edge_index = torch.tensor([\n",
    "            [0, 1, 2],\n",
    "            [0, 1, 2]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        data['institution', 'hosts', 'author'].edge_index = torch.tensor([\n",
    "            [0, 1],\n",
    "            [0, 1]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    batch_size = 4\n",
    "    sequence_size = 5\n",
    "    \n",
    "    # Create a batch of sequences\n",
    "    graphs_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        sequence = [create_dummy_heterodata() for _ in range(sequence_size)]\n",
    "        graphs_batch.append(sequence)\n",
    "    \n",
    "    # Move to device if needed\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = encoder.to(device)\n",
    "    \n",
    "    # Optionally, move data to device\n",
    "    # This requires iterating and moving each HeteroData to the device\n",
    "    for i in range(batch_size):\n",
    "        for j in range(sequence_size):\n",
    "            for key in graphs_batch[i][j].x_dict.keys():\n",
    "                graphs_batch[i][j].x_dict[key] = graphs_batch[i][j].x_dict[key].to(device)\n",
    "            for key in graphs_batch[i][j].edge_index_dict.keys():\n",
    "                graphs_batch[i][j].edge_index_dict[key] = graphs_batch[i][j].edge_index_dict[key].to(device)\n",
    "    \n",
    "    # Encode the batch\n",
    "    with torch.no_grad():\n",
    "        latent_representations = encoder(graphs_batch)  # Shape: (batch_size, sequence_size, 128)\n",
    "    \n",
    "    print(latent_representations.shape)  # Should print torch.Size([4, 5, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8072611e-d007-48fb-9b1c-7fe005245f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print(arg):\n",
    "    print(f\"{arg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48a1b460-a990-4f9e-8e62-6b77cfd1e15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg='kuk'\n"
     ]
    }
   ],
   "source": [
    "kuk = 34\n",
    "debug_print(\"kuk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c3cb1f3-7a6e-4837-9941-68af2be54607",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = torch.tensor((\n",
    "    [\n",
    "    [1,2,3],[4,5,6], [1,2,3],[4,5,6]\n",
    "    ],\n",
    "    [\n",
    "    [11,12,13],[41,53,60],[31,42,53],[46,5,16]\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69bba970-0c80-4152-809f-5f10acf1f168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc0a7f9b-60a2-40b7-a5e1-f2007e4d7e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 14, 16],\n",
       "        [45, 58, 66],\n",
       "        [32, 44, 56],\n",
       "        [50, 10, 22]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e8494-9bd1-4278-a769-7418d5fee513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    epoch_0 = 0\n",
    "    for epoch in range(epoch_0, self.autoencoder_num_epochs):\n",
    "                epoch_loss = []\n",
    "                print(\"Experiment: autoencoder {0}: training Epoch = \".format(self.autoencoder), epoch+1, 'out of', self.autoencoder_num_epochs, 'epochs')\n",
    "    \n",
    "                # Loop through all the train data using the data loader\n",
    "                for ii, (dem, ob, ac, l, t, scores, rewards, idx) in enumerate(self.train_loader):\n",
    "                    # print(\"Batch {}\".format(ii),end='')\n",
    "                    dem = dem.to(device)  # 5 dimensional vector (Gender, Ventilation status, Re-admission status, Age, Weight)\n",
    "                    ob = ob.to(device)    # 33 dimensional vector (time varying measures)\n",
    "                    ac = ac.to(device) # actions\n",
    "                    l = l.to(device)\n",
    "                    t = t.to(device)\n",
    "                    scores = scores.to(device)\n",
    "                    idx = idx.to(device)\n",
    "                    loss_pred = 0\n",
    "    \n",
    "                    # Cut tensors down to the batch's largest sequence length... Trying to speed things up a bit...\n",
    "                    max_length = int(l.max().item())\n",
    "    \n",
    "                    # The following losses are for DDM and will not be modified by any other approach\n",
    "                    train_loss, dec_loss, inv_loss = 0, 0, 0\n",
    "                    model_loss, recon_loss, forward_loss = 0, 0, 0                    \n",
    "                        \n",
    "                    # Set training mode (nn.Module.train()). It does not actually trains the model, but just sets the model to training mode.\n",
    "                    self.gen.train()\n",
    "                    self.pred.train()\n",
    "    \n",
    "                    ob = ob[:,:max_length,:]\n",
    "                    dem = dem[:,:max_length,:]\n",
    "                    ac = ac[:,:max_length,:]\n",
    "                    scores = scores[:,:max_length,:]\n",
    "                    \n",
    "                    # Special case for CDE\n",
    "                    # Getting loss_pred and mse_loss\n",
    "                    if self.autoencoder == 'CDE':\n",
    "                        loss_pred, mse_loss, _ = self.container.loop(ob, dem, ac, scores, l, max_length, self.context_input, corr_coeff_param = self.corr_coeff_param, device = device, coefs = self.train_coefs, idx = idx)\n",
    "                    else:\n",
    "                        loss_pred, mse_loss, _ = self.container.loop(ob, dem, ac, scores, l, max_length, self.context_input, corr_coeff_param = self.corr_coeff_param, device=device, autoencoder = self.autoencoder)   \n",
    "    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    if self.autoencoder != 'DDM':\n",
    "                        loss_pred.backward()\n",
    "                        self.optimizer.step()\n",
    "                        epoch_loss.append(loss_pred.detach().cpu().numpy())                \n",
    "                    else:\n",
    "                        # For DDM\n",
    "                        train_loss, dec_loss, inv_loss, model_loss, recon_loss, forward_loss, corr_loss, loss_pred = loss_pred\n",
    "                        train_loss = forward_loss + self.inv_loss_coef*inv_loss + self.dec_loss_coef*dec_loss - self.corr_coeff_param*corr_loss.sum()\n",
    "                        train_loss.backward()\n",
    "                        # Clipping gradients to prevent exploding gradients\n",
    "                        torch.nn.utils.clip_grad_norm(self.all_params, self.max_grad_norm)\n",
    "                        self.optimizer.step()\n",
    "                        epoch_loss.append(loss_pred.detach().cpu().numpy())\n",
    "                                            \n",
    "                self.autoencoding_losses.append(epoch_loss)\n",
    "                if (epoch+1)%self.saving_period == 0: # Run validation and also save checkpoint\n",
    "                    \n",
    "                    #Computing validation loss\n",
    "                    epoch_validation_loss = []\n",
    "                    with torch.no_grad():\n",
    "                        for jj, (dem, ob, ac, l, t, scores, rewards, idx) in enumerate(self.val_loader):\n",
    "    \n",
    "                            dem = dem.to(device)\n",
    "                            ob = ob.to(device)\n",
    "                            ac = ac.to(device)\n",
    "                            l = l.to(device)\n",
    "                            t = t.to(device)\n",
    "                            idx = idx.to(device)\n",
    "                            scores = scores.to(device)\n",
    "                            loss_val = 0\n",
    "    \n",
    "                            # Cut tensors down to the batch's largest sequence length... Trying to speed things up a bit...\n",
    "                            max_length = int(l.max().item())                        \n",
    "                            \n",
    "                            ob = ob[:,:max_length,:]\n",
    "                            dem = dem[:,:max_length,:]\n",
    "                            ac = ac[:,:max_length,:] \n",
    "                            scores = scores[:,:max_length,:] \n",
    "                            \n",
    "                            self.gen.eval()\n",
    "                            self.pred.eval()    \n",
    "                            \n",
    "                            if self.autoencoder == 'CDE':\n",
    "                                loss_val, mse_loss, _ = self.container.loop(ob, dem, ac, scores, l, max_length, corr_coeff_param = 0, device = device, coefs = self.val_coefs, idx = idx)\n",
    "                            else:\n",
    "                                loss_val, mse_loss, _ = self.container.loop(ob, dem, ac, scores, l, max_length, self.context_input, corr_coeff_param = 0, device=device, autoencoder = self.autoencoder)                                                 \n",
    "                            \n",
    "                            if self.autoencoder in ['DST', 'ODERNN', 'CDE']:\n",
    "                                epoch_validation_loss.append(mse_loss)\n",
    "                            elif self.autoencoder == \"DDM\":\n",
    "                                epoch_validation_loss.append(loss_val[-1].detach().cpu().numpy())\n",
    "                            else:\n",
    "                                epoch_validation_loss.append(loss_val.detach().cpu().numpy())\n",
    "                        \n",
    "                            \n",
    "                    self.autoencoding_losses_validation.append(epoch_validation_loss)\n",
    "    \n",
    "                    save_dict = {'epoch': epoch,\n",
    "                            'gen_state_dict': self.gen.state_dict(),\n",
    "                            'pred_state_dict': self.pred.state_dict(),\n",
    "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                            'loss': self.autoencoding_losses,\n",
    "                            'validation_loss': self.autoencoding_losses_validation\n",
    "                            }\n",
    "                    \n",
    "                    if self.autoencoder == 'DDM':\n",
    "                        save_dict['dyn_state_dict'] = self.dyn.state_dict()\n",
    "                        \n",
    "                    try:\n",
    "                        torch.save(save_dict, self.checkpoint_file)\n",
    "                        # torch.save(save_dict, self.checkpoint_file[:-3] + str(epoch) +'_.pt')\n",
    "                        np.save(self.data_folder + '/{}_losses.npy'.format(self.autoencoder.lower()), np.array(self.autoencoding_losses))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "    \n",
    "                    \n",
    "                    try:\n",
    "                        np.save(self.data_folder + '/{}_validation_losses.npy'.format(self.autoencoder.lower()), np.array(self.autoencoding_losses_validation))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        \n",
    "                #Final epoch checkpoint\n",
    "                try:\n",
    "                    save_dict = {\n",
    "                                'epoch': self.autoencoder_num_epochs-1,\n",
    "                                'gen_state_dict': self.gen.state_dict(),\n",
    "                                'pred_state_dict': self.pred.state_dict(),\n",
    "                                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                                'loss': self.autoencoding_losses,\n",
    "                                'validation_loss': self.autoencoding_losses_validation,\n",
    "                                }\n",
    "                    if self.autoencoder == 'DDM':\n",
    "                        save_dict['dyn_state_dict'] = self.dyn.state_dict()\n",
    "                        torch.save(self.dyn.state_dict(), self.dyn_file)\n",
    "                    torch.save(self.gen.state_dict(), self.gen_file)\n",
    "                    torch.save(self.pred.state_dict(), self.pred_file)\n",
    "                    torch.save(save_dict, self.checkpoint_file)\n",
    "                    np.save(self.data_folder + '/{}_losses.npy'.format(self.autoencoder.lower()), np.array(self.autoencoding_losses))\n",
    "                except Exception as e:\n",
    "                        print(e)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f2ca7-a635-4e38-aeb2-20e843f8eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.node_time_dict = {\n",
    "    \"user\": torch.tensor([1, 3, 5, 7]),  # Timestamps for user nodes\n",
    "    \"item\": torch.tensor([2, 4, 6, 8]),  # Timestamps for item nodes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be726955-af7a-4c98-8f10-094388cae55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeit\n",
    "def time_select_up_to_t(dataset, t):\n",
    "    \"\"\"Select nodes and edges up to and including time step t.\n",
    "    @param t: time index (inclusive)\n",
    "    @return: HeteroData, with sliced nodes and edges based on time.\n",
    "    \"\"\"\n",
    "    dt = HeteroData()\n",
    "    d = dataset\n",
    "\n",
    "    # Get node timestamps (if they exist)\n",
    "    node_time_dict = getattr(d, \"node_time_dict\", {})  # Use empty dict if node_time_dict doesn't exist\n",
    "\n",
    "    # Copy node information up to and including time step t\n",
    "    for ntype, value in d.x_dict.items():\n",
    "        if ntype in node_time_dict:  # If node type has time information\n",
    "            mask = (node_time_dict[ntype] <= t).squeeze(-1)  # Filter nodes based on time\n",
    "            dt[ntype].x = value[mask]  # Copy node features for filtered nodes\n",
    "            if \"num_nodes\" in d[ntype].keys():\n",
    "                dt[ntype].num_nodes = mask.sum().item()  # Update number of nodes\n",
    "        else:  # If node type has no time information, copy all nodes\n",
    "            dt[ntype].x = value\n",
    "            if \"num_nodes\" in d[ntype].keys():\n",
    "                dt[ntype].num_nodes = d[ntype].num_nodes\n",
    "\n",
    "    # Get edge timestamps and edge indices\n",
    "    dea = d.edge_time_dict\n",
    "    dei = d.edge_index_dict\n",
    "\n",
    "    # Filter edges up to and including time step t\n",
    "    for etype in dea:\n",
    "        mask = (dea[etype] <= t).squeeze(-1)  # Include all edges with time <= t\n",
    "        dt[etype].edge_index = dei[etype][:, mask]\n",
    "        dt[etype].edge_time = dea[etype][mask]\n",
    "\n",
    "    return dt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name",
   "language": "python",
   "name": "my-virtualenv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
